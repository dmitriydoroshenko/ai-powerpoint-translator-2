import os
import json
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

SYSTEM_ROLE = (
    "## Role\n"
    "You are an expert Game Localization (L10N) Specialist and professional mobile game localizer. "
    "Your goal is to translate English mobile gaming market reports and game text into Simplified Chinese, "
    "ensuring the output is natural and uses industry-standard jargon used by developers and publishers.\n\n"

    "## Task\n"
    "1. Input: A JSON array of objects with 'id' and 'xml'.\n"
    "2. Action: Translate ONLY the text content within <a:t> tags in the provided XML to Simplified Chinese.\n"
    "3. Output: Return a JSON object with a key 'translations' containing the array of objects, "
    "each having the original 'id' and the 'translated_text' containing the modified XML.\n\n"

    "## Terminology & Style Guidelines\n"
    "- Do Not Translate Game Titles: Keep all game names/titles in their original English form.\n"
    "- Avoid Literalism: Do not translate word-for-word. Focus on industry 'jargon.'\n"
    "- Spending/Monetization:\n"
    "  * 'Non-paying players' -> Èùû‰ªòË¥πÁé©ÂÆ∂ / Èõ∂Ê∞™Áé©ÂÆ∂\n"
    "  * 'Spending real money' -> ‰ªòË¥π / Ê∞™Èáë\n"
    "- Events & Scheduling:\n"
    "  * 'Global schedule' -> ÂÖ®ÊúçÁªü‰∏ÄÊó•Á®ã / Âõ∫ÂÆöÊ°£Êúü\n"
    "  * 'Progress in events' -> Êé®ËøõÊ¥ªÂä®ËøõÂ∫¶\n"
    "- Tone: Professional, concise, and analytical. Use 'Game-speak.'\n"
)

def translate_all(texts, batch_size=10):
    if not texts:
        print("‚ùå –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –ø—É—Å—Ç.")
        return []

    start_time = time.perf_counter()
    total_texts = len(texts)

    batches = [texts[i:i + batch_size] for i in range(0, total_texts, batch_size)]
    total_batches = len(batches)
    
    results = [None] * total_texts
    total_prompt_tokens = 0
    total_completion_tokens = 0
    
    print(f"\n{'='*40}")
    print(f"üöÄ –ó–ê–ü–£–°–ö –ü–ï–†–ï–í–û–î–ê (–ë–∞—Ç—á–∏–Ω–≥: {batch_size} —Å—Ç—Ä/–∑–∞–ø—Ä–æ—Å)")
    print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_texts} | –ë–∞—Ç—á–µ–π: {total_batches}")
    print(f"{'='*40}\n")

    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_batch = {
            executor.submit(_translate_batch, [(i + start, texts[i + start]) 
            for i in range(len(batches[b_idx]))]): b_idx 
            for b_idx, start in enumerate(range(0, total_texts, batch_size))
        }
        
        completed_batches = 0
        for future in as_completed(future_to_batch):
            batch_results, usage = future.result()
            
            for idx, trans_text in batch_results:
                results[idx] = trans_text
            
            if usage:
                total_prompt_tokens += usage.prompt_tokens
                total_completion_tokens += usage.completion_tokens
            
            completed_batches += 1

            percent = (completed_batches / total_batches) * 100
            print(f"‚è≥ –ë–∞—Ç—á {completed_batches}/{total_batches} –∑–∞–≤–µ—Ä—à–µ–Ω ({percent:.1f}%) | "
                  f"–°—Ç—Ä–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {min(completed_batches * batch_size, total_texts)}")

    end_time = time.perf_counter()
    duration = end_time - start_time
    minutes, seconds = divmod(int(duration), 60)

    cost = (total_prompt_tokens * 1.75 / 1_000_000) + (total_completion_tokens * 14.00 / 1_000_000)

    print(f"\n{'='*40}")
    print(f"‚úÖ –ü–ï–†–ï–í–û–î –ó–ê–í–ï–†–®–ï–ù")
    print(f"‚è± –í—Ä–µ–º—è: {minutes} –º–∏–Ω. {seconds} —Å–µ–∫.")
    print(f"üìä –¢–æ–∫–µ–Ω—ã: –ü—Ä–æ–º–ø—Ç: {total_prompt_tokens} | –û—Ç–≤–µ—Ç: {total_completion_tokens}")
    print(f"üí∞ –û—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${cost:.4f}")
    print(f"{'='*40}\n")

    return results

def _translate_batch(batch):
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π [(index, text), ...]
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ [(index, translated_text), ...] –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ–∫–µ–Ω–æ–≤.
    """
    try:
        payload = [{"id": idx, "xml": text} for idx, text in batch]
        
        response = client.chat.completions.create(
            model="gpt-5.2",
            messages=[
                {"role": "system", "content": SYSTEM_ROLE},
                {"role": "user", "content": json.dumps(payload, ensure_ascii=False)}
            ],
            response_format={"type": "json_object"},
            temperature=0.1
        )

        raw_content = response.choices[0].message.content
        
        if raw_content is None:
            print("‚ö†Ô∏è API –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç (None)")
            return [(idx, text) for idx, text in batch], response.usage

        content = json.loads(raw_content)
        translated_data = content.get("translations", [])
        
        translations_map = {item['id']: item['translated_text'] for item in translated_data}
        
        result_batch = []
        for idx, original_text in batch:
            translated_text = translations_map.get(idx, original_text)
            result_batch.append((idx, translated_text))
            
        return result_batch, response.usage

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞: {e}")
        return [(idx, text) for idx, text in batch], None